{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark 101\n",
    "\n",
    "### An introduction to distributed computing\n",
    "\n",
    "October 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Installing PySpark\n",
    "\n",
    "Steps for Windows:\n",
    "1. Make sure you have java with `java -version`\n",
    "2. Make sure you have Python with `python --version`\n",
    "3. Download Spark from http://spark.apache.org/downloads.html\n",
    "4. Unzip files with tar xvzf spark-3.0.0-bin-hadoop2.7.tgz\n",
    "5. Set environment varibles with (you need to be running as admin):\n",
    "    - setx SPARK_HOME \"%USERPROFILE%\\Documents\\spark\\spark-3.0.0-bin-hadoop2.7\" /M\n",
    "    - setx HADOOP_HOME \"%USERPROFILE%\\Documents\\spark\\spark-3.0.0-bin-hadoop2.7\" /M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributed Computing\n",
    "For processing vast volumes of data fast, we need to **scale out** instead of **scale up**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Cheaper**: Run large data on clusters of many nodes (i.e. smaller and cheaper machines.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Faster**: It parallelizes and distributes computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Reliable**: If one node or process fails, its workload is assumed by other components in the system. (Also known as fault tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark Apache\n",
    "Open-source distributed cluster-computing framework. It extends the MapReduce model using RDDs (Resilient Distributed Datasets).\n",
    "\n",
    "Available in Java, Scala, Python, R. (PySpark is the Python distribution.)\n",
    "\n",
    "**Key Components:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Distribution**: Distribute the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Parallelism**: Perform subsets of the computation simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Fault tolerance**: Handle component failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MapReduce\n",
    "Let’s remember that MapReduce is a programming model for processing big data sets with a parallel, distributed algorithm on a cluster. \n",
    "\n",
    "- **Map procedure**: Applies a function to each data point over a partition in parallel. \n",
    "Examples: `filter()`, `map()`\n",
    "- **Reduce procedure**: Summary operation that returns one value from multiple values\n",
    "Examples: `reduce()`, `sum()`, `count()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='img/example.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# It helps you find spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext() \n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "\n",
    "ss.catalog.clearCache()\n",
    "sc.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Read data as an RDD\n",
    "rdd = sc.textFile(\"data/numbers.txt\", 8) # Number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# See data on the computer\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Logical cores = (# of physical cores) x (# of threads that can run on each core)\n",
    "\n",
    "`sysctl -n hw.ncpu`\n",
    "\n",
    "`rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# A note about lambda functions\n",
    "\n",
    "def add_2(x):\n",
    "    return x+2\n",
    "\n",
    "add_2_lambda = lambda x: x+2\n",
    "\n",
    "assert add_2(3) == add_2_lambda(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now let's convert data to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example of a map function\n",
    "converted_rdd = rdd.map(lambda x: int(x))\n",
    "converted_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='img/string2int.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now let's filter to get numbers greater or equal to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example of another map function\n",
    "filtered_rdd = converted_rdd.filter(lambda x: x >= 8)\n",
    "filtered_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='img/greater8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now let's add numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example of a reduce function\n",
    "filtered_rdd.reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**What's happening?** The rdd is adding two numbers in the same partition until there is only one number left. Then it adds two numbers in different partitions until there is only one numebr left (shuffling happens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example of another reduce function\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Part 1: MapReduce -- Summary\n",
    "Sum of integers that are greater than or equal to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read data as an RDD\n",
    "rdd = sc.textFile(\"data/numbers.txt\", 8) # Number of cores\n",
    "\n",
    "converted_rdd = rdd.map(lambda x: int(x)) # Map function that converts to integers\n",
    "filtered_rdd = converted_rdd.filter(lambda x: x >= 8) # Map function to filter numbers\n",
    "\n",
    "filtered_rdd.glom().collect() # To see results in partitions\n",
    "\n",
    "filtered_rdd.reduce(lambda x, y: x+y) # Reduce function to add integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Other useful functions\n",
    "\n",
    "rdd.getNumPartitions() # Get number of partitions\n",
    "rdd.count() # Reduce function to count how many elements are in the original rdd\n",
    "rdd.first() # Get first object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"ex01_output\") # Saves it into parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2: RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "Abstraction of a distributed collection of items with operations and transformation applicable to the dataset.\n",
    "\n",
    "They are:\n",
    "- Distributed\n",
    "- Immutable\n",
    "- Resilient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RDD Operations\n",
    "\n",
    "- **Transformations**:\n",
    "    - Perform functions against each element in an RDD and return a new RDD\n",
    "    - *Lazy evaluation*: Operations are only evaluated when an action is requested\n",
    "- **Actions**\n",
    "    - Trigger a computation and return a value to the Spark driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RDD Operations - Transformations\n",
    "\n",
    "<img src=\"img/transformations.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RDD Operations - Actions\n",
    "\n",
    "<img src=\"img/actions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Excercise: Calculate the sum of the odd numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc.stop() # Don't forget to close your connection"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
